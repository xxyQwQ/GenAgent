- `CLIPTextEncodeSDXL`: This node is designed to refine and encode text inputs using the CLIP model, specifically tailored for the SDXL architecture. It enhances textual descriptions for image generation or manipulation by incorporating aesthetic scores and dimensions, leveraging the CLIP model's understanding of text in visual contexts.
    - Parameters:
        - `width`: The desired width of the output image, affecting the aspect ratio and detail level of the generated visual content. Type should be `INT`.
        - `height`: The desired height of the output image, impacting the aspect ratio and detail resolution of the generated visuals. Type should be `INT`.
        - `crop_w`: The width of the crop area from the original image, which can influence the focus area of the generated image. Type should be `INT`.
        - `crop_h`: The height of the crop area from the original image, affecting the focus area of the generated image. Type should be `INT`.
        - `target_width`: The target width for the output image, which may differ from the original width to adjust the image's aspect ratio or detail. Type should be `INT`.
        - `target_height`: The target height for the output image, potentially altering the original height to adjust the image's aspect ratio or detail. Type should be `INT`.
        - `text_g`: The global textual input that provides the overall descriptive context for image generation, encoded by the CLIP model. Type should be `STRING`.
        - `text_l`: The local textual input that provides specific details for refining the image generation, encoded by the CLIP model. Type should be `STRING`.
    - Inputs:
        - `clip`: An instance of the CLIP model used for encoding the text input into a visual context-aware representation. Type should be `CLIP`.
    - Outputs:
        - `conditioning`: The output is a conditioning format that includes the encoded text representation along with additional metadata necessary for image generation or manipulation tasks. Type should be `CONDITIONING`.
